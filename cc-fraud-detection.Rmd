---
title: "Credit Card Fraud Detection Kaggle"
author: "Andrea Ranzato"
date: "11/1/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warnings = FALSE, 
                      fig.align = "center")
```

# Motivation

(WORK IN PROGRESS)

I use the Credit Card Fraud Detection data set available on [Kaggle.com](https://www.kaggle.com/mlg-ulb/creditcardfraud) in order to practice the
the Tidymodels meta-package.
The objective of the this analysis is to train a classifier able to predict credit card fraudulent transactions.

The [screencast](https://www.youtube.com/watch?v=9f6t5vaNyEM) by [Julia Silge](https://juliasilge.com) was extremely helpful in guiding me through the steps of the Tidymodels framework.

```{r packages}
library(tidyverse)
library(knitr)
library(scales)
library(lubridate)
theme_set(theme_light())
```

## Content

This section is pasted from [https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud).

The datasets contains transactions made by credit cards in September 2013 by european cardholders. 
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.

It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.

## Explore data

```{r dataset}
df <- read_csv("data/creditcard.csv")
skimr::skim(df)
```

```{r numb-trans-minute}
# Divide Time in bins of 60 seconds
trans_min <- df %>% 
  group_by(seconds = 60 * (Time %/% 60 + 1)) %>% 
  summarise(n_transaction = n(), 
            fraud = sum(Class), 
            not_fraud = n() - sum(Class),
            perc_fraud = mean(Class)*100,
            perc_not_fraud = (1 - mean(Class))*100) %>% 
  mutate(minute = seconds/60) %>% 
  relocate(minute, .after = seconds) 

```
```{r transactions-graph}
trans_min %>% 
  select(seconds, fraud, not_fraud) %>% 
  pivot_longer(fraud:not_fraud, names_to = "Class", values_to = "n") %>% 
  mutate(Class = if_else(Class == "fraud", "Fraud", "Not Fraud"),
         seconds = hms::hms(seconds)) %>% 
  ggplot(aes(seconds, n, colour = Class)) +
  geom_line(size = 0.15) +
  scale_x_time(breaks = scales::breaks_width("12 hour")) +
  #scale_y_continuous(limits = c(0, 300)) +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5)) +
  labs(y = "Count",
       x = "Hour",
       title = "Number of transactions per minute.")
```

Seasonality in the datetime and night-time.

```{r histograms-amounts}
df %>% 
  select(-contains("V")) %>% 
  mutate(Class = as.character(Class),
         Class = if_else(Class == "1", "Fraud", "Not Fraud")) %>% 
  #filter(Class == "Fraud") %>% 
  ggplot(aes(Amount, fill = Class)) +
  geom_histogram(position = "identity", alpha = 0.7, bins = 100) +
  scale_x_continuous(labels = scales::label_dollar()) +
  scale_y_continuous(labels = scales::comma_format()) +
  facet_grid(Class ~ ., scales = "free") +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none") +
  labs(title = "Distribution of the transaction amounts in ($).", x = "")
```


```{r boxplots-amounts}
df %>% 
  select(-contains("V")) %>% 
  mutate(Class = as.character(Class),
         Class = if_else(Class == "1", "Fraud", "Not Fraud")) %>% 
  #filter(Class == "Fraud") %>% 
  ggplot(aes(Class, Amount, fill = Class)) +
  geom_boxplot() +
  scale_y_continuous(labels = scales::label_dollar()) +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none") +
  labs(title = "Boxplots of the transaction amounts in ($).") +
  coord_flip()
```
```{r}
df %>% 
  mutate(Class = if_else(Class == "1", "Fraud", "Not Fraud")) %>% 
  group_by(Class) %>% 
  summarise(min_amt = min(Amount), 
            first_q_amt = quantile(Amount, 0.25),
            median_amt = median(Amount), 
            mean_amt = mean(Amount), 
            third_q_amt = quantile(Amount, 0.75),
            max_amt = max(Amount),
            sd = sd(Amount)) %>% 
  kable(digits = 2, format.args = list(big.mark = ",", scientific = FALSE),
        col.names = c("Class", "Min.", "1st Q.", "Median", "Mean", "3rd Q.", "Max.", "Sd."))
```


```{r NAs}
df %>% 
  summarise(across(everything(), ~ sum(is.na(.)))) %>% 
  pivot_longer(everything(), names_to = "Feature", values_to = "NAs_Numb") %>% 
  filter(NAs_Numb != 0)
# No NA detected
```
```{r target-variable}
target <- df %>% 
  mutate(Class = if_else(Class == 0, "Not Fraud", "Fraud")) %>% 
  group_by(Class) %>% 
  summarise(n = n(), perc = (n()/nrow(df))*100)

target %>% 
  ggplot(aes(x = Class, y = perc, fill = Class)) +
  geom_bar(stat = "identity") +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none") +
  labs(y = "%",
       title = "Class imbalance of the target variable.")
```


```{r target-varible-table}
 target %>% 
  kable(digits = 2, format.args = list(big.mark = ",", scientific = FALSE),
        caption = "Frequency of Frauds on Credit Cards.",
        col.names = c("Class", "Total", "Percentage"))
```

## Build models

Since the target variable *Class* is affected by class imbalance, we use stratified random sampling in order to keep the proportion of fraudulent transaction similar in both train and test set.

```{r}
library(tidymodels)

df <- df %>% 
  mutate(Class = factor(Class))

# Train and test with stratified random sampling
set.seed(1234)
trans_split <- initial_split(df, strata = Class)
trans_train <- training(trans_split)
trans_test <- testing(trans_split) 

set.seed(345)

# Create cross validations folds from the training
# used to make choices.
# Empirical method to evaluate models.
trans_folds <- vfold_cv(trans_train)
```

```{r}
library(themis)
trans_rec <- recipe(Class ~ ., data = trans_train) %>% 
  step_smote(Class)
  # Synthetic observations.
  # It makes new examples of fraudulent transactions in order to have the same 
  # proportion of 0 and 1.
  # By doing so, enanched learning process.

trans_wf <- workflow() %>% 
  add_recipe(trans_rec)

trans_wf

```
```{r models-engines}
glm_spec <- logistic_reg() %>% 
  set_engine("glm")

rf_spec <- rand_forest(trees = 100) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")
```

```{r glm-train, cache=TRUE}
doParallel::registerDoParallel()

# Not strong reasons to tune
glm_rs <- trans_wf %>% 
  add_model(glm_spec) %>% 
  fit_resamples(
    resamples = trans_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
    control = control_resamples(save_pred = TRUE)
  )
glm_rs

```
```{r rf-train, cache=TRUE}
doParallel::registerDoParallel()

# Not strong reasong to tune
rf_rs <- trans_wf %>% 
  add_model(rf_spec) %>% 
  fit_resamples(
    resamples = trans_folds,
    metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
    control = control_resamples(save_pred = TRUE)
  )
rf_rs
```

## Evaluate models

```{r metrics-conf-mat}
collect_metrics(glm_rs)
collect_metrics(rf_rs)

glm_rs %>% 
  conf_mat_resampled()

rf_rs %>% 
  conf_mat_resampled()
```

```{r roc-logistic-regression}
glm_rs %>% 
  collect_predictions() %>% 
  group_by(id) %>% 
  roc_curve(Class, .pred_0) %>% 
  autoplot()
```

```{r roc-random-forest}
rf_rs %>% 
  collect_predictions() %>% 
  group_by(id) %>% 
  roc_curve(Class, .pred_0) %>% 
  autoplot()
```

 
```{r whole-training}
# Train and evaluate on testinting data
trans_final <- trans_wf %>% 
  add_model(glm_spec) %>% 
  last_fit(trans_split)

# Metrics on testing data
collect_metrics(trans_final)

collect_predictions(trans_final) %>% 
  conf_mat(Class, .pred_class)
```

```{r final-model}
trans_final %>% 
  pull(.workflow) %>% 
  pluck(1) %>% 
  # Odds ratios
  tidy(exponentiate = TRUE) %>% 
  # First are predictors that make the prob of a trans. being fraudolent lower
  arrange(estimate) %>% 
  kable(digits = 3)

trans_final %>% 
  pull(.workflow) %>% 
  pluck(1) %>% 
  # Odds ratios
  tidy() %>% 
  filter(term != "(Intercept)") %>% 
  ggplot(aes(estimate, fct_reorder(term, estimate))) +
  geom_vline(xintercept = 0, colour = "grey50", lty = 2, size = 1.2) +
  geom_errorbar(aes(xmin = estimate - std.error,
                    xmax = estimate + std.error),
                width = 0.2, alpha = 0.7) +
  geom_point()

# On the positive side are the predictors that increase the likelihood of making a transaction fraudulent

# On the negative side are the predictors that decrease the likelihood of making a transaction fraudulent
```

